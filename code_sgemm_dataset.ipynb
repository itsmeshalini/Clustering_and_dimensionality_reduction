{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BUAN6341: Assignment 3 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyodbc \n",
    "import multiprocessing\n",
    "import os\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import learning_curve, cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error,confusion_matrix, roc_curve,roc_auc_score, accuracy_score, homogeneity_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import FastICA, PCA\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import learning_curve, cross_val_score, train_test_split,GridSearchCV,learning_curve, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda deactivate  \n",
    "\n",
    "pip install tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning Libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import initializers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset 1 **  \n",
    "_Data pre-processing and cleaning_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"sgemm_product.csv\")\n",
    "df1.describe()\n",
    "df1['y']=(df1['Run1 (ms)']+df1['Run2 (ms)']+df1['Run3 (ms)']+df1['Run4 (ms)'])/4\n",
    "\n",
    "#print(df['y'])\n",
    "# remove extra fields in data set\n",
    "df1.drop(['Run1 (ms)', 'Run2 (ms)','Run3 (ms)', 'Run4 (ms)'], axis=1, inplace=True)\n",
    "\n",
    "target_col = ['y']\n",
    "matrix_col = ['MWG','NWG']\n",
    "dimn_col = ['KWG']\n",
    "wkgsize_col = ['MDIMC','NDIMC']\n",
    "memshape_col = ['MDIMA','NDIMB']\n",
    "kloop_col = ['KWI']\n",
    "vecwidth_col = ['VWM','VWN']\n",
    "enstride_col = ['STRM','STRN']\n",
    "cach_col = ['SA','SB']\n",
    "\n",
    "dep_var = target_col\n",
    "indep_var = matrix_col + dimn_col+wkgsize_col+memshape_col+kloop_col+vecwidth_col+enstride_col+cach_col\n",
    "df1.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove outliers\n",
    "\n",
    "df1 = df1[~(np.abs(df1['y']-df1['y'].mean()) > (3*df1['y'].std()))]#count= 5251 = 2.17% of data\n",
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.sample(n=100000, random_state=1111)\n",
    "df1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting dataset into X and y \n",
    "X_Emp = df1.iloc[:,1:].values\n",
    "y_Emp = df1.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "full_xscaler = StandardScaler().fit(X_Emp)\n",
    "x_scaled_Emp = full_xscaler.transform(X_Emp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled_Emp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=.80,random_state=123456)\n",
    "x_scaled_pca_Emp = pca.fit_transform(x_scaled_Emp)\n",
    "x_scaled_pca_Emp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Means Clustering with all feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "\n",
    "for i in range(1,16):\n",
    "    Emp_kmeans = KMeans(n_clusters = i,n_jobs = -1,n_init = 10,max_iter = 300,random_state = 0)\n",
    "    Emp_kmeans.fit(x_scaled_Emp)\n",
    "    wcss.append(Emp_kmeans.inertia_)\n",
    "    \n",
    "plt.plot(range(1,16),wcss)\n",
    "plt.xlabel('number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.title('Using Elbow method to get the error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best K means with all features Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emp_kmeans = KMeans(n_clusters = 4,n_jobs = -1,n_init = 10,max_iter = 300)\n",
    "y_Emp_kmeans = Emp_kmeans.fit_predict(x_scaled_Emp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_Emp_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_cent = Emp_kmeans.cluster_centers_\n",
    "kmeans_cent\n",
    "kmeans_cent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_Emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 4\n",
    "kmeans_cent = Emp_kmeans.cluster_centers_\n",
    "colors = ['#FF0000','#0000FF','#00ff00','#ffff00','#008000','#00fff9' ,'#800080','#FFA500','#16e739','#ff00f4','#FFFF00', '#000000' ,'#FFD700','#808000','#b266b2','#E3CF57']\n",
    "labels = Emp_kmeans.labels_\n",
    "col_map=dict(zip(set(labels),colors))\n",
    "label_color = [col_map[l] for l in labels]\n",
    "col1 =0\n",
    "col2 =1\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "for i in range(n_clusters):\n",
    "    plt.scatter(x_scaled_Emp[labels==i, col1],x_scaled_Emp[labels==i, col2],marker='.', s=30, lw=0, alpha=0.9, color = colors[i],edgecolor='k')\n",
    "    plt.scatter(kmeans_cent[i, col1], kmeans_cent[i, col2], c=colors[i], marker='o',alpha=1, s=200, edgecolor='k', linewidths=2,label = f'Cluster {i}')\n",
    "    plt.xlabel(f'axis {col1}')\n",
    "    plt.ylabel(f'axis {col2}')\n",
    "    plt.legend(loc='best')\n",
    "plt.title(f'Scaled Dataset with {n_clusters} clusters along with All Features' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Means Clustering on feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best Decision tree\n",
    "deci_tree = tree.DecisionTreeClassifier(max_depth=7, criterion = 'entropy')\n",
    "deci_tree.fit(x_scaled_Emp,y_Emp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled_Emp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exclude_cols = ['y']\n",
    "deci_tree_impor = pd.DataFrame(deci_tree.feature_importances_, index=[df1[df1.columns[~df1.columns.isin(exclude_cols)]].columns], columns=['importance']).sort_values(by='importance', ascending=False)\n",
    "deci_tree_impor = deci_tree_impor.reset_index().rename(columns={'level_0': 'features'}, level=0)\n",
    "imp_feats = deci_tree_impor[deci_tree_impor.importance > 0].features.tolist()\n",
    "deci_tree_impor[deci_tree_impor.importance > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find rows where feature importance is greater than 0\n",
    "x_scaled_Emp_deciimpfeatures = x_scaled_Emp[:,deci_tree.feature_importances_ > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Finding optimum no of clusters to be used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss_imp_features = []\n",
    "\n",
    "for i in range(1,16):\n",
    "    Emp_impfeatures_kmeans = KMeans(n_clusters = i,n_jobs = -1,n_init = 10,max_iter = 300,random_state = 0)\n",
    "    Emp_impfeatures_kmeans.fit(x_scaled_Emp_deciimpfeatures)\n",
    "    wcss_imp_features.append(Emp_impfeatures_kmeans.inertia_)\n",
    "    \n",
    "plt.plot(range(1,16),wcss_imp_features)\n",
    "plt.xlabel('number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.title('Using Elbow method to get the error for the best features using Decision trees K Means')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best K Means feature selection via Decissin Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emp_impfeatures_kmeans = KMeans(n_clusters = 7,n_jobs = -1,n_init = 10,max_iter = 300)\n",
    "Emp_impfeatures_kmeans.fit_predict(x_scaled_Emp_deciimpfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_impfeatures_center = Emp_impfeatures_kmeans.cluster_centers_\n",
    "colors = ['#FF0000','#0000FF','#00ff00','#ffff00','#008000','#00fff9' ,'#800080','#FFA500','#16e739','#ff00f4','#FFFF00', '#000000' ,'#FFD700','#808000','#b266b2']\n",
    "labels = Emp_impfeatures_kmeans.labels_\n",
    "color_matrix=dict(zip(set(labels),colors))\n",
    "label_color = [color_matrix[l] for l in labels]\n",
    "col1 =0\n",
    "col2 =1\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "for i in range(0,6):\n",
    "    plt.scatter(x_scaled_Emp_deciimpfeatures[labels==i, col1],x_scaled_Emp_deciimpfeatures[labels==i, col2],marker='.', s=30, lw=0, alpha=0.9, color = colors[i],edgecolor='k')\n",
    "    plt.scatter(kmeans_impfeatures_center[i, col1], kmeans_impfeatures_center[i, col2], c=colors[i], marker='o',alpha=1, s=200, edgecolor='k', linewidths=2,label = f'Cluster {i}')\n",
    "    plt.xlabel(f'axis {col1}')\n",
    "    plt.ylabel(f'axis {col2}')\n",
    "    plt.legend(loc='best')\n",
    "plt.title(f'Dataset with 6 for best features extracted using decision tree' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction -PCA- K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss_pca = []\n",
    "\n",
    "for i in range(1,16):\n",
    "    Emp_pca_kmeans = KMeans(n_clusters = i,n_jobs = -1,n_init = 10,max_iter = 300,random_state = 0)\n",
    "    Emp_pca_kmeans.fit(x_scaled_pca_Emp)\n",
    "    wcss_pca.append(Emp_pca_kmeans.inertia_)\n",
    "    \n",
    "plt.plot(range(1,16),wcss_pca)\n",
    "plt.xlabel('number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.title('Using Elbow method to get the error for the PCA K Means')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bset K-Means With All features Scaled and PCA applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emp_pca_kmeans = KMeans(n_clusters = 6,n_jobs = -1,n_init = 10,max_iter = 300)\n",
    "Emp_pca_kmeans.fit_predict(x_scaled_pca_Emp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pca_center = Emp_pca_kmeans.cluster_centers_\n",
    "colors = ['#FF0000','#0000FF','#00ff00','#ffff00','#008000','#00fff9' ,'#800080','#FFA500','#16e739','#ff00f4','#FFFF00', '#000000' ,'#FFD700','#808000','#b266b2']\n",
    "labels = Emp_pca_kmeans.labels_\n",
    "color_matrix=dict(zip(set(labels),colors))\n",
    "label_color = [color_matrix[l] for l in labels]\n",
    "col1 =0\n",
    "col2 =1\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "for i in range(0,6):\n",
    "    plt.scatter(x_scaled_pca_Emp[labels==i, col1],x_scaled_pca_Emp[labels==i, col2],marker='.', s=30, lw=0, alpha=0.9, color = colors[i],edgecolor='k')\n",
    "    plt.scatter(kmeans_pca_center[i, col1], kmeans_pca_center[i, col2], c=colors[i], marker='o',alpha=1, s=200, edgecolor='k', linewidths=2,label = f'Cluster {i}')\n",
    "    plt.xlabel(f'axis {col1}')\n",
    "    plt.ylabel(f'axis {col2}')\n",
    "    plt.legend(loc='best')\n",
    "plt.title(f'Dataset with 7 Scaled All Features and PCA applied' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reductions- ICA -K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ICA\n",
    "random_state = 12345\n",
    "ica = FastICA(n_components=15, random_state=random_state)\n",
    "x_scaled_ica_Emp = ica.fit_transform(x_scaled_Emp)\n",
    "ica.mixing_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wcss_ica = []\n",
    "\n",
    "for i in range(1,20):\n",
    "    Emp_ica_kmeans = KMeans(n_clusters = i,n_jobs = -1,n_init = 10,max_iter = 300,random_state = 0)\n",
    "    Emp_ica_kmeans.fit(x_scaled_ica_Emp)\n",
    "    wcss_ica.append(Emp_ica_kmeans.inertia_)\n",
    "    \n",
    "plt.plot(range(1,20),wcss_ica)\n",
    "plt.xlabel('number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.title('Using Elbow method to get the error for the ICA K Means')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best K MEANS with all features scaled and ICA applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emp_ica_kmeans = KMeans(n_clusters = 7,n_jobs = -1,n_init = 10,max_iter = 300)\n",
    "Emp_ica_kmeans.fit_predict(x_scaled_ica_Emp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_ica_center = Emp_ica_kmeans.cluster_centers_\n",
    "colors = ['#FF0000','#0000FF','#00ff00','#ffff00','#008000','#00fff9' ,'#800080','#FFA500','#16e739','#ff00f4','#FFFF00', '#000000' ,'#FFD700','#808000','#b266b2']\n",
    "labels = Emp_ica_kmeans.labels_\n",
    "color_matrix=dict(zip(set(labels),colors))\n",
    "label_color = [color_matrix[l] for l in labels]\n",
    "col1 =0\n",
    "col2 =1\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "for i in range(0,7):\n",
    "    plt.scatter(x_scaled_ica_Emp[labels==i, col1],x_scaled_ica_Emp[labels==i, col2],marker='.', s=30, lw=0, alpha=0.9, color = colors[i],edgecolor='k')\n",
    "    plt.scatter(kmeans_ica_center[i, col1], kmeans_ica_center[i, col2], c=colors[i], marker='o',alpha=1, s=200, edgecolor='k', linewidths=2,label = f'Cluster {i}')\n",
    "    plt.xlabel(f'axis {col1}')\n",
    "    plt.ylabel(f'axis {col2}')\n",
    "    plt.legend(loc='best')\n",
    "plt.title(f'Dataset with 12 Scaled All Features and ICA applied' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Dimensionality Reduction -RCA- KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute RCA\n",
    "rca = GaussianRandomProjection(n_components = 20, random_state= random_state)\n",
    "x_scaled_rca_Emp= rca.fit_transform(x_scaled_Emp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss_rca = []\n",
    "\n",
    "for i in range(1,16):\n",
    "    Emp_rca_kmeans = KMeans(n_clusters = i,n_jobs = -1,n_init = 10,max_iter = 300,random_state = 0)\n",
    "    Emp_rca_kmeans.fit(x_scaled_rca_Emp)\n",
    "    wcss_rca.append(Emp_rca_kmeans.inertia_)\n",
    "    \n",
    "plt.plot(range(1,16),wcss_rca)\n",
    "plt.xlabel('number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.title('Using Elbow method to get the error for the RCA K Means')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emp_rca_kmeans = KMeans(n_clusters = 4,n_jobs = -1,n_init = 10,max_iter = 300)\n",
    "Emp_rca_kmeans.fit_predict(x_scaled_rca_Emp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_rca_center = Emp_rca_kmeans.cluster_centers_\n",
    "colors = ['#FF0000','#0000FF','#00ff00','#ffff00','#008000','#00fff9' ,'#800080','#FFA500','#16e739','#ff00f4','#FFFF00', '#000000' ,'#FFD700','#808000','#b266b2']\n",
    "labels = Emp_rca_kmeans.labels_\n",
    "color_matrix=dict(zip(set(labels),colors))\n",
    "label_color = [color_matrix[l] for l in labels]\n",
    "col1 =0\n",
    "col2 =1\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "for i in range(0,4):\n",
    "    plt.scatter(x_scaled_rca_Emp[labels==i, col1],x_scaled_rca_Emp[labels==i, col2],marker='.', s=30, lw=0, alpha=0.9, color = colors[i],edgecolor='k')\n",
    "    plt.scatter(kmeans_rca_center[i, col1], kmeans_rca_center[i, col2], c=colors[i], marker='o',alpha=1, s=200, edgecolor='k', linewidths=2,label = f'Cluster {i}')\n",
    "    plt.xlabel(f'axis {col1}')\n",
    "    plt.ylabel(f'axis {col2}')\n",
    "    plt.legend(loc='best')\n",
    "plt.title(f'Dataset with 4 Scaled All Features and RCA applied' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Expectation Minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bic = []\n",
    "aic = []\n",
    "convergence_types = ['spherical', 'tied', 'diag', 'full']   \n",
    "for j in convergence_types:\n",
    "    for i in range(1,16):\n",
    "        # Fit a Gaussian mixture with EM\n",
    "        gmm = GaussianMixture(n_components=i,covariance_type=j, n_init = 10)\n",
    "        gmm.fit(x_scaled_Emp)\n",
    "        bic.append(gmm.bic(x_scaled_Emp))\n",
    "        aic.append(gmm.aic(x_scaled_Emp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "range_ = range(1, 16)\n",
    "plt.figure(figsize = (10,8))\n",
    "for i, convergence_type in enumerate(convergence_types):\n",
    "    plt.plot(list(range_),bic[i*len(range_):(i+1)*len(range_)],linestyle = '--', label = f'{convergence_type} - AIC')\n",
    "    plt.plot(list(range_),aic[i*len(range_):(i+1)*len(range_)],label = f'{convergence_type} - BIC')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('AIC & BIC Scores')\n",
    "    plt.ylabel('AIC & BIC Scores')\n",
    "    plt.xlabel('number of clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Expectation minimization Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emp_gmm = GaussianMixture(n_components=7 ,covariance_type='full',n_init=10)\n",
    "Emp_gmm_full = Emp_gmm.fit(x_scaled_Emp).predict(x_scaled_Emp)\n",
    "means = Emp_gmm.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#FF0000','#0000FF','#00ff00','#ffff00','#008000','#00fff9' ,'#800080','#FFA500','#16e739','#ff00f4','#FFFF00', '#000000' ,'#FFD700','#808000','#b266b2']\n",
    "col_map=dict(zip(set(Emp_gmm_full),colors))\n",
    "label_color = [col_map[l] for l in Emp_gmm_full]\n",
    "col1 = 0\n",
    "col2 = 1\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "for i in range(0,7):\n",
    "    plt.scatter(x_scaled_Emp[Emp_gmm_full==i, col1],x_scaled_Emp[Emp_gmm_full==i, col2],marker='.', s=30, lw=0, alpha=0.7, color = colors[i],edgecolor='k')\n",
    "    plt.scatter(means[i, col1], means[i, col2], c=colors[i], marker='o',alpha=1, s=200, edgecolor='k', linewidths=2, label = f'Cluster {i}')\n",
    "    plt.xlabel(f'axis {col1}')\n",
    "    plt.ylabel(f'axis {col2}')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "\n",
    "plt.title(f'Dataset with 7 Scaled Expec Max and all features' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation maximization on feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bic_impfeatures = []\n",
    "aic_impfeatures = []\n",
    "convergence_types = ['spherical', 'tied', 'diag', 'full']   \n",
    "for j in convergence_types:\n",
    "    for i in range(1,9):\n",
    "        # Fit a Gaussian mixture with EM\n",
    "        gmm_impfeatures = GaussianMixture(n_components=i,covariance_type=j, n_init = 8)\n",
    "        gmm_impfeatures.fit(x_scaled_Emp_deciimpfeatures)\n",
    "        bic_impfeatures.append(gmm.bic(x_scaled_Emp_deciimpfeatures))\n",
    "        aic_impfeatures.append(gmm.aic(x_scaled_Emp_deciimpfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_ = range(1, 16)\n",
    "plt.figure(figsize = (10,8))\n",
    "for i, convergence_type in enumerate(convergence_types):\n",
    "    plt.plot(list(range_),bic_impfeatures[i*len(range_):(i+1)*len(range_)],linestyle = '--', label = f'{convergence_type} - AIC')\n",
    "    plt.plot(list(range_),aic_impfeatures[i*len(range_):(i+1)*len(range_)],label = f'{convergence_type} - BIC')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('AIC & BIC Scores')\n",
    "    plt.ylabel('AIC & BIC Scores')\n",
    "    plt.xlabel('number of clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Expectation Maximization on important feature selected dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emp_gmm_imp = GaussianMixture(n_components=6 ,covariance_type='full',n_init=10)\n",
    "Emp_gmm_impfeatures = Emp_gmm_imp.fit(x_scaled_Emp_deciimpfeatures).predict(x_scaled_Emp_deciimpfeatures)\n",
    "means_impfeatures = Emp_gmm_imp.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colors = ['#FF0000','#0000FF','#00ff00','#ffff00','#008000','#00fff9' ,'#800080','#FFA500','#16e739','#ff00f4','#FFFF00', '#000000' ,'#FFD700','#808000','#b266b2']\n",
    "col_map=dict(zip(set(Emp_gmm_impfeatures),colors))\n",
    "label_color = [col_map[l] for l in Emp_gmm_impfeatures]\n",
    "col1 = 0\n",
    "col2 = 1\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "for i in range(0,6):\n",
    "    plt.scatter(x_scaled_Emp_deciimpfeatures[Emp_gmm_impfeatures==i, col1],x_scaled_Emp_deciimpfeatures[Emp_gmm_impfeatures==i, col2],marker='.', s=30, lw=0, alpha=0.7, color = colors[i],edgecolor='k')\n",
    "    plt.scatter(means_impfeatures[i, col1], means_impfeatures[i, col2], c=colors[i], marker='o',alpha=1, s=200, edgecolor='k', linewidths=2, label = f'Cluster {i}')\n",
    "    plt.xlabel(f'axis {col1}')\n",
    "    plt.ylabel(f'axis {col2}')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "\n",
    "plt.title(f'Dataset with 6 Scaled Expec Max and all features' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction -PCA -Expectation Maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bic_pca = []\n",
    "aic_pca = []\n",
    "convergence_types = ['spherical', 'tied', 'diag', 'full']   \n",
    "for j in convergence_types:\n",
    "    for i in range(1,16):\n",
    "        # Fit a Gaussian mixture with EM\n",
    "        gmm_pca = GaussianMixture(n_components=i,covariance_type=j, n_init = 10)\n",
    "        gmm_pca.fit(x_scaled_pca_Emp)\n",
    "        bic_pca.append(gmm_pca.bic(x_scaled_pca_Emp))\n",
    "        aic_pca.append(gmm_pca.aic(x_scaled_pca_Emp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_ = range(1, 16)\n",
    "plt.figure(figsize = (10,8))\n",
    "for i, convergence_type in enumerate(convergence_types):\n",
    "    plt.plot(list(range_),bic_pca[i*len(range_):(i+1)*len(range_)],linestyle = '--', label = f'{convergence_type} - AIC')\n",
    "    plt.plot(list(range_),aic_pca[i*len(range_):(i+1)*len(range_)],label = f'{convergence_type} - BIC')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('AIC & BIC Scores')\n",
    "    plt.ylabel('AIC & BIC Scores')\n",
    "    plt.xlabel('number of clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Expectation-Maximization on PCA applied Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emp_gmm_pca = GaussianMixture(n_components=6 ,covariance_type='full',n_init=10)\n",
    "Emp_gmm_pcafeatures = Emp_gmm_pca.fit(x_scaled_pca_Emp).predict(x_scaled_pca_Emp)\n",
    "means_pcafeatures = Emp_gmm_pca.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#FF0000','#0000FF','#00ff00','#ffff00','#008000','#00fff9' ,'#800080','#FFA500','#16e739','#ff00f4','#FFFF00', '#000000' ,'#FFD700','#808000','#b266b2']\n",
    "col_map=dict(zip(set(Emp_gmm_pcafeatures),colors))\n",
    "label_color = [col_map[l] for l in Emp_gmm_pcafeatures]\n",
    "col1 = 0\n",
    "col2 = 1\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "for i in range(0,6):\n",
    "    plt.scatter(x_scaled_pca_Emp[Emp_gmm_pcafeatures==i, col1],x_scaled_pca_Emp[Emp_gmm_pcafeatures==i, col2],marker='.', s=30, lw=0, alpha=0.7, color = colors[i],edgecolor='k')\n",
    "    plt.scatter(means_pcafeatures[i, col1], means_pcafeatures[i, col2], c=colors[i], marker='o',alpha=1, s=200, edgecolor='k', linewidths=2, label = f'Cluster {i}')\n",
    "    plt.xlabel(f'axis {col1}')\n",
    "    plt.ylabel(f'axis {col2}')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "\n",
    "plt.title(f'Dataset with 6 Scaled Expec Max and all features' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reductions - ICA - Expectation Maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bic_ica = []\n",
    "aic_ica = []\n",
    "convergence_types = ['spherical', 'tied', 'diag', 'full']   \n",
    "for j in convergence_types:\n",
    "    for i in range(1,16):\n",
    "        # Fit a Gaussian mixture with EM\n",
    "        gmm_ica = GaussianMixture(n_components=i,covariance_type=j, n_init = 10)\n",
    "        gmm_ica.fit(x_scaled_ica_Emp)\n",
    "        bic_ica.append(gmm_ica.bic(x_scaled_ica_Emp))\n",
    "        aic_ica.append(gmm_ica.aic(x_scaled_ica_Emp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_ = range(1, 16)\n",
    "plt.figure(figsize = (10,8))\n",
    "for i, convergence_type in enumerate(convergence_types):\n",
    "    plt.plot(list(range_),bic_ica[i*len(range_):(i+1)*len(range_)],linestyle = '--', label = f'{convergence_type} - AIC')\n",
    "    plt.plot(list(range_),aic_ica[i*len(range_):(i+1)*len(range_)],label = f'{convergence_type} - BIC')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('AIC & BIC Scores')\n",
    "    plt.ylabel('AIC & BIC Scores')\n",
    "    plt.xlabel('number of clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Expectation-Maximization on ICA applied Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emp_gmm_ica = GaussianMixture(n_components=6 ,covariance_type='full',n_init=10)\n",
    "Emp_gmm_icafeatures = Emp_gmm_ica.fit(x_scaled_ica_Emp).predict(x_scaled_ica_Emp)\n",
    "means_icafeatures = Emp_gmm_ica.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#FF0000','#0000FF','#00ff00','#ffff00','#008000','#00fff9' ,'#800080','#FFA500','#16e739','#ff00f4','#FFFF00', '#000000' ,'#FFD700','#808000','#b266b2']\n",
    "col_map=dict(zip(set(Emp_gmm_icafeatures),colors))\n",
    "label_color = [col_map[l] for l in Emp_gmm_icafeatures]\n",
    "col1 = 0\n",
    "col2 = 1\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "for i in range(0,6):\n",
    "    plt.scatter(x_scaled_ica_Emp[Emp_gmm_icafeatures==i, col1],x_scaled_ica_Emp[Emp_gmm_icafeatures==i, col2],marker='.', s=30, lw=0, alpha=0.7, color = colors[i],edgecolor='k')\n",
    "    plt.scatter(means_icafeatures[i, col1], means_icafeatures[i, col2], c=colors[i], marker='o',alpha=1, s=200, edgecolor='k', linewidths=2, label = f'Cluster {i}')\n",
    "    plt.xlabel(f'axis {col1}')\n",
    "    plt.ylabel(f'axis {col2}')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "\n",
    "plt.title(f'Dataset with 6 Scaled Expec Max and all features' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reductions - RCA - Expectation Maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bic_rca = []\n",
    "aic_rca = []\n",
    "convergence_types = ['spherical', 'tied', 'diag', 'full']   \n",
    "for j in convergence_types:\n",
    "    for i in range(1,16):\n",
    "        # Fit a Gaussian mixture with EM\n",
    "        gmm_rca = GaussianMixture(n_components=i,covariance_type=j, n_init = 10)\n",
    "        gmm_rca.fit(x_scaled_rca_Emp)\n",
    "        bic_rca.append(gmm_rca.bic(x_scaled_rca_Emp))\n",
    "        aic_rca.append(gmm_rca.aic(x_scaled_rca_Emp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_ = range(1, 16)\n",
    "plt.figure(figsize = (10,8))\n",
    "for i, convergence_type in enumerate(convergence_types):\n",
    "    plt.plot(list(range_),bic_rca[i*len(range_):(i+1)*len(range_)],linestyle = '--', label = f'{convergence_type} - AIC')\n",
    "    plt.plot(list(range_),aic_rca[i*len(range_):(i+1)*len(range_)],label = f'{convergence_type} - BIC')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('AIC & BIC Scores')\n",
    "    plt.ylabel('AIC & BIC Scores')\n",
    "    plt.xlabel('number of clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Expectation-Maximization on RCA applied Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emp_gmm_rca = GaussianMixture(n_components=6 ,covariance_type='full',n_init=10)\n",
    "Emp_gmm_rcafeatures = Emp_gmm_rca.fit(x_scaled_rca_Emp).predict(x_scaled_rca_Emp)\n",
    "means_rcafeatures = Emp_gmm_rca.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#FF0000','#0000FF','#00ff00','#ffff00','#008000','#00fff9' ,'#800080','#FFA500','#16e739','#ff00f4','#FFFF00', '#000000' ,'#FFD700','#808000','#b266b2']\n",
    "col_map=dict(zip(set(Emp_gmm_rcafeatures),colors))\n",
    "label_color = [col_map[l] for l in Emp_gmm_rcafeatures]\n",
    "col1 = 0\n",
    "col2 = 1\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "for i in range(0,6):\n",
    "    plt.scatter(x_scaled_rca_Emp[Emp_gmm_rcafeatures==i, col1],x_scaled_rca_Emp[Emp_gmm_rcafeatures==i, col2],marker='.', s=30, lw=0, alpha=0.7, color = colors[i],edgecolor='k')\n",
    "    plt.scatter(means_rcafeatures[i, col1], means_rcafeatures[i, col2], c=colors[i], marker='o',alpha=1, s=200, edgecolor='k', linewidths=2, label = f'Cluster {i}')\n",
    "    plt.xlabel(f'axis {col1}')\n",
    "    plt.ylabel(f'axis {col2}')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "\n",
    "plt.title(f'Dataset with 6 Scaled Expec Max and all features' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Clusters for Dataset Using Homogenity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = target_col\n",
    "\n",
    "homo_scores = []\n",
    "names = []\n",
    "homo_scores.append(homogeneity_score(y,Emp_kmeans.labels_))\n",
    "names.append('kmeans AllFeatures')\n",
    "\n",
    "homo_scores.append(homogeneity_score(y,Emp_impfeatures_kmeans.labels_))\n",
    "names.append('kmeans ImpFeat')\n",
    "\n",
    "homo_scores.append(homogeneity_score(y,Emp_pca_kmeans.labels_))\n",
    "names.append('kmeans PCA')\n",
    "\n",
    "homo_scores.append(homogeneity_score(y,Emp_ica_kmeans.labels_))\n",
    "names.append('kmeans ICA')\n",
    "\n",
    "homo_scores.append(homogeneity_score(y,Emp_rca_kmeans.labels_))\n",
    "names.append('kmeans RCA')\n",
    "\n",
    "homo_scores.append(homogeneity_score(y,Emp_gmm_full))\n",
    "names.append('ExpecMax AllFeatures')\n",
    "\n",
    "homo_scores.append(homogeneity_score(y,Emp_gmm_impfeatures))\n",
    "names.append('ExpecMax ImpFeat')\n",
    "\n",
    "homo_scores.append(homogeneity_score(y,Emp_gmm_pcafeatures))\n",
    "names.append('ExpecMax PCA')\n",
    "\n",
    "homo_scores.append(homogeneity_score(y,Emp_gmm_icafeatures))\n",
    "names.append('ExpecMax ICA')\n",
    "\n",
    "homo_scores.append(homogeneity_score(y,Emp_gmm_rcafeatures))\n",
    "names.append('ExpecMax RCA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "y_ = np.arange(len(names))\n",
    "plt.bar(y_,homo_scores, align='edge')\n",
    "plt.xticks(y_, names,rotation = 75)\n",
    "plt.ylabel('Homogenity Scores')\n",
    "plt.title('Comparison of Various Clustering Type on Datasets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the accuracy rate for all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {'scaled_dataset': x_scaled_Emp, 'Imp_feat_dataset':x_scaled_Emp_deciimpfeatures, 'PCA_dataset':x_scaled_pca_Emp, \n",
    "            'ICA_dataset':x_scaled_ica_Emp,  'RCA_dataset':x_scaled_rca_Emp, 'labels_dataset':Emp_kmeans.labels_}\n",
    "y = target_col\n",
    "model_results = []\n",
    "\n",
    "cv = 8\n",
    "nodes_hidden_layer1 = 120\n",
    "nn_cv = []\n",
    "kfold = KFold(n_splits=cv,shuffle=True)\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "for key, dataset in datasets.items():\n",
    "    if key == 'labels_dataset':\n",
    "        x_train, x_test, y_train, y_test = train_test_split(dataset, y, test_size = .20, random_state = 0)\n",
    "        \n",
    "        try:\n",
    "            y_test_row, y_test_classes = y_test.shape\n",
    "        except ValueError:\n",
    "            y_test_row = y_test.shape[0]\n",
    "            y_test_classes = 1\n",
    "            \n",
    "        x_train = x_train[:,np.newaxis]\n",
    "        y_train = y_train[:,np.newaxis]\n",
    "        for train, test in kfold.split(x_train,y_train):\n",
    "            # earling stopping defined\n",
    "            es = EarlyStopping(monitor='val_acc', mode='max')\n",
    "            # Initializing the ANN\n",
    "            classifier = Sequential()\n",
    "            #Adding the input layer and the first hidden layer\n",
    "            classifier.add(Dense(nodes_hidden_layer1, input_shape=(x_train.shape[1],),activation='softmax', kernel_initializer=initializers.random_normal(mean = 0.0, stddev=0.02)))\n",
    "            for i in range(1, 3):\n",
    "                nodes_hidden_layer1 = np.ceil(nodes_hidden_layer1/2).astype('int32')\n",
    "                classifier.add(Dense(nodes_hidden_layer1, activation='relu', kernel_initializer=initializers.random_normal(mean = 0.0, stddev=0.02)))\n",
    "\n",
    "                # Adding the output layer\n",
    "            classifier.add(Dense(y_test_classes, activation='sigmoid'))\n",
    "\n",
    "                # Compiling the ANN\n",
    "            classifier.compile(loss='binary_crossentropy',optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "                # Fitting the ANN to the training set\n",
    "            history = classifier.fit(x_train, y_train, validation_data=(x_test, y_test),epochs=100, batch_size = 400, verbose=0, callbacks=[es])\n",
    "\n",
    "                # evaluate the model\n",
    "            train_loss, train_acc = classifier.evaluate(x_train, y_train, verbose=0)\n",
    "            test_loss, test_acc = classifier.evaluate(x_test, y_test, verbose=0)\n",
    "            nn_cv.append(test_acc)\n",
    "        model_results.append(np.array(nn_cv))\n",
    "            \n",
    "    else:\n",
    "        x_train, x_test, y_train, y_test = train_test_split(dataset, y, test_size = .20, shuffle = True)\n",
    "        try:\n",
    "            y_test_row, y_test_classes = y_test.shape\n",
    "        except ValueError:\n",
    "            y_test_row = y_test.shape[0]\n",
    "            y_test_classes = 1\n",
    "        for train, test in kfold.split(x_train,y_train):\n",
    "            # earling stopping defined\n",
    "            es = EarlyStopping(monitor='val_acc', mode='max')\n",
    "            # Initializing the ANN\n",
    "            classifier = Sequential()\n",
    "            #Adding the input layer and the first hidden layer\n",
    "            classifier.add(Dense(nodes_hidden_layer1, input_shape=(x_train.shape[1],),activation='softmax', kernel_initializer=initializers.random_normal(mean = 0.0, stddev=0.02)))\n",
    "            for i in range(1, 3):\n",
    "                nodes_hidden_layer1 = np.ceil(nodes_hidden_layer1/2).astype('int32')\n",
    "                classifier.add(Dense(nodes_hidden_layer1, activation='relu', kernel_initializer=initializers.random_normal(mean = 0.0, stddev=0.02)))\n",
    "\n",
    "                # Adding the output layer\n",
    "            classifier.add(Dense(y_test_classes, activation='sigmoid'))\n",
    "\n",
    "                # Compiling the ANN\n",
    "            classifier.compile(loss='binary_crossentropy',optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "                # Fitting the ANN to the training set\n",
    "            history = classifier.fit(x_train, y_train, validation_data=(x_test, y_test),epochs=100, batch_size = 400, verbose=0, callbacks=[es])\n",
    "\n",
    "                # evaluate the model\n",
    "            train_loss, train_acc = classifier.evaluate(x_train, y_train, verbose=0)\n",
    "            test_loss, test_acc = classifier.evaluate(x_test, y_test, verbose=0)\n",
    "            nn_cv.append(test_acc)\n",
    "        model_results.append(np.array(nn_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['scaled_dataset', 'Imp_feat_dataset','PCA_dataset', 'ICA_dataset','RCA_dataset','labels_dataset']\n",
    "fig = plt.figure(figsize=(15,12))\n",
    "fig.suptitle('COMPARISON OF VARIOUS MODELS ON EMPLOYEE ATTRITION DATASET')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(model_results)\n",
    "ax.set_xticklabels(models)\n",
    "ax.set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
